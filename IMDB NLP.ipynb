{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d44bf5",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cc66ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ee1df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Hiren/Desktop/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e407784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "095e805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace({'negative':0,'positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "583947ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "305d7d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=data['sentiment'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0208e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hiren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean the text by removing any extra characters \n",
    "import re # tools for cleaning text\n",
    "import nltk # library to perform NLP\n",
    "nltk.download('stopwords') # tool to remove the non-essential words. It is a list of words\n",
    "from nltk.corpus import stopwords\n",
    "# Stemming means taking the root of the word such as love from loved or loving\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afab6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(range(7000,50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1a93666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       7000\n",
       "sentiment    7000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a83d0e",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "575f3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['review'][0]\n",
    "review = re.sub('[^a-zA-Z]', ' ', data2['review'][0]) # only keep the letters so what we dont want to remove\n",
    "review = review.lower() # make all letters as lowercase \n",
    "review = review.split()  # split the review into separate words to create a list of words\n",
    "ps =PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not  word in set(stopwords.words('english'))] # use set for larger text\n",
    "review = ' '.join(review) # take the list of words back into the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7776102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one review mention watch oz episod hook right exactli happen br br first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word br br call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death stare dodgi deal shadi agreement never far away br br would say main appeal show due fact goe show dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw struck nasti surreal say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a9e9261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all the reviews in the dataset using the for loop\n",
    "corpus = [] # Build a new list for all the words from all the reviews\n",
    "# corpus is a collection of text and a common term used in NLP\n",
    "for i in range (0, 7000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data2['review'][i] )\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps =PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not  word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21c94541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl time favorit movi stori selfless sacrific dedic nobl caus preachi bore never get old despit seen time last year paul luka perform bring tear eye bett davi one truli sympathet role delight kid grandma say like dress midget children make fun watch mother slow awaken happen world roof believ startl dozen thumb movi'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5ac39",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "### Bag of Words Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77b947a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the bag of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features= 1500)# We restrict the words to 1500 which keeps the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6596446",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus).toarray()# independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef338c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39e2e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abil', 'abl', 'absolut', 'absurd', 'abus', 'academi', 'accent', 'accept', 'accid', 'achiev', 'across', 'act', 'action', 'actor', 'actress', 'actual', 'ad', 'adam', 'adapt', 'add', 'addit', 'admir', 'admit', 'adult', 'adventur', 'affair', 'affect', 'afraid', 'african', 'age', 'agent', 'ago', 'agre', 'ahead', 'air', 'al', 'alien', 'aliv', 'allow', 'almost', 'alon', 'along', 'alreadi', 'also', 'although', 'alway', 'amaz', 'america', 'american', 'among', 'amount', 'amus', 'angel', 'angl', 'angri', 'anim', 'ann', 'annoy', 'anoth', 'answer', 'anti', 'anymor', 'anyon', 'anyth', 'anyway', 'apart', 'appar', 'appeal', 'appear', 'appreci', 'approach', 'area', 'arm', 'armi', 'around', 'arriv', 'art', 'artist', 'asid', 'ask', 'aspect', 'assum', 'atmospher', 'attack', 'attempt', 'attent', 'attitud', 'attract', 'audienc', 'author', 'avail', 'averag', 'avoid', 'aw', 'award', 'away', 'awesom', 'babi', 'back', 'background', 'bad', 'badli', 'ball', 'band', 'bar', 'bare', 'base', 'basic', 'batman', 'battl', 'bear', 'beat', 'beauti', 'becam', 'becom', 'bed', 'began', 'begin', 'behind', 'believ', 'ben', 'besid', 'best', 'better', 'beyond', 'big', 'biggest', 'bill', 'billi', 'bit', 'bizarr', 'black', 'blame', 'blind', 'blond', 'blood', 'blow', 'blue', 'board', 'boat', 'bodi', 'bomb', 'bond', 'book', 'bore', 'born', 'boss', 'bother', 'bottom', 'bought', 'box', 'boy', 'boyfriend', 'br', 'brain', 'break', 'brian', 'brief', 'brilliant', 'bring', 'british', 'brother', 'brought', 'brown', 'bruce', 'brutal', 'buddi', 'budget', 'build', 'bunch', 'burn', 'busi', 'buy', 'call', 'came', 'cameo', 'camera', 'camp', 'cannot', 'captain', 'captur', 'car', 'care', 'career', 'carri', 'cartoon', 'case', 'cast', 'cat', 'catch', 'caught', 'caus', 'celebr', 'center', 'central', 'centuri', 'certain', 'certainli', 'cgi', 'challeng', 'chanc', 'chang', 'channel', 'charact', 'charl', 'charli', 'charm', 'chase', 'cheap', 'check', 'cheesi', 'chemistri', 'chick', 'child', 'childhood', 'children', 'chill', 'chines', 'choic', 'choos', 'chri', 'christian', 'christma', 'christoph', 'church', 'cinema', 'cinemat', 'cinematographi', 'citi', 'claim', 'class', 'classic', 'clear', 'clearli', 'clever', 'clich', 'climax', 'close', 'cloth', 'club', 'clue', 'co', 'cold', 'collect', 'colleg', 'color', 'combin', 'come', 'comedi', 'comic', 'command', 'comment', 'commentari', 'commerci', 'commit', 'common', 'commun', 'compani', 'compar', 'comparison', 'compel', 'complet', 'complex', 'comput', 'concept', 'concern', 'conclus', 'condit', 'conflict', 'confus', 'connect', 'consid', 'consist', 'constantli', 'contain', 'content', 'continu', 'control', 'convers', 'convey', 'convinc', 'cool', 'cop', 'copi', 'core', 'cost', 'costum', 'could', 'count', 'countri', 'coupl', 'cours', 'cover', 'craft', 'crap', 'crash', 'crazi', 'creat', 'creativ', 'creatur', 'credit', 'creepi', 'crew', 'cri', 'crime', 'crimin', 'critic', 'cross', 'crowd', 'cult', 'cultur', 'current', 'cut', 'cute', 'dad', 'damn', 'danc', 'danger', 'danni', 'dare', 'dark', 'date', 'daughter', 'davi', 'david', 'day', 'de', 'dead', 'deal', 'death', 'decad', 'decent', 'decid', 'decis', 'deep', 'definit', 'delight', 'deliv', 'demand', 'demon', 'depict', 'depress', 'depth', 'describ', 'desert', 'deserv', 'design', 'desir', 'desper', 'despit', 'destroy', 'detail', 'detect', 'develop', 'devil', 'dialog', 'dialogu', 'die', 'differ', 'difficult', 'direct', 'director', 'disappear', 'disappoint', 'disast', 'discov', 'discuss', 'disgust', 'disney', 'display', 'disturb', 'doctor', 'documentari', 'dog', 'dollar', 'done', 'door', 'doubl', 'doubt', 'dr', 'drag', 'dragon', 'drama', 'dramat', 'draw', 'drawn', 'dread', 'dream', 'dress', 'drink', 'drive', 'drop', 'drug', 'dub', 'due', 'dull', 'dumb', 'dvd', 'earli', 'earlier', 'earth', 'easi', 'easili', 'eat', 'ed', 'eddi', 'edg', 'edit', 'educ', 'effect', 'effort', 'either', 'element', 'els', 'embarrass', 'emot', 'encount', 'end', 'enemi', 'energi', 'engag', 'english', 'enjoy', 'enough', 'enter', 'entertain', 'entir', 'epic', 'episod', 'equal', 'era', 'escap', 'especi', 'essenti', 'establish', 'etc', 'even', 'event', 'eventu', 'ever', 'everi', 'everybodi', 'everyon', 'everyth', 'evid', 'evil', 'ex', 'exactli', 'exampl', 'excel', 'except', 'excit', 'excus', 'execut', 'exist', 'expect', 'experi', 'explain', 'explan', 'exploit', 'explor', 'express', 'extra', 'extrem', 'eye', 'face', 'fact', 'fail', 'fair', 'fairli', 'faith', 'fake', 'fall', 'famili', 'familiar', 'famou', 'fan', 'fantasi', 'fantast', 'far', 'fascin', 'fashion', 'fast', 'fat', 'father', 'favorit', 'fear', 'featur', 'feel', 'fell', 'fellow', 'felt', 'femal', 'festiv', 'fi', 'fiction', 'field', 'fight', 'figur', 'fill', 'film', 'filmmak', 'final', 'find', 'fine', 'finish', 'fire', 'first', 'fish', 'fit', 'five', 'flash', 'flashback', 'flat', 'flaw', 'fli', 'flick', 'floor', 'focu', 'focus', 'folk', 'follow', 'fool', 'footag', 'forc', 'ford', 'forev', 'forget', 'form', 'former', 'fortun', 'forward', 'found', 'four', 'frame', 'frank', 'free', 'french', 'fresh', 'friend', 'friendship', 'front', 'frustrat', 'full', 'fulli', 'fun', 'funni', 'funniest', 'futur', 'gag', 'game', 'gang', 'gangster', 'garbag', 'gave', 'gay', 'gem', 'gener', 'geniu', 'genr', 'genuin', 'georg', 'german', 'get', 'ghost', 'giant', 'girl', 'girlfriend', 'give', 'given', 'glad', 'go', 'god', 'goe', 'gold', 'gone', 'good', 'gore', 'gorgeou', 'got', 'govern', 'grace', 'grade', 'grand', 'grant', 'graphic', 'great', 'greatest', 'green', 'ground', 'group', 'grow', 'guard', 'guess', 'gun', 'guy', 'hair', 'half', 'hand', 'handl', 'hang', 'happen', 'happi', 'hard', 'hardli', 'harri', 'hate', 'haunt', 'head', 'hear', 'heard', 'heart', 'heaven', 'heavi', 'held', 'hell', 'help', 'henri', 'hero', 'heroin', 'hey', 'hide', 'high', 'highli', 'highlight', 'hilari', 'hill', 'hire', 'histor', 'histori', 'hit', 'hitchcock', 'hold', 'hole', 'hollywood', 'holm', 'home', 'honest', 'honestli', 'hook', 'hope', 'horribl', 'horror', 'hors', 'hospit', 'hot', 'hotel', 'hour', 'hous', 'howev', 'huge', 'human', 'humor', 'humour', 'hundr', 'hunt', 'hurt', 'husband', 'ice', 'idea', 'ident', 'idiot', 'ignor', 'ii', 'ill', 'imag', 'imagin', 'imdb', 'immedi', 'import', 'imposs', 'impress', 'improv', 'includ', 'incred', 'inde', 'independ', 'indian', 'individu', 'industri', 'influenc', 'inform', 'initi', 'innoc', 'insan', 'insid', 'inspir', 'instead', 'insult', 'intellig', 'intend', 'intens', 'intent', 'interest', 'intern', 'interview', 'intrigu', 'introduc', 'investig', 'involv', 'iron', 'island', 'issu', 'italian', 'jack', 'jackson', 'jame', 'jane', 'japanes', 'jason', 'jean', 'jerri', 'jim', 'jimmi', 'joan', 'job', 'joe', 'john', 'join', 'joke', 'jone', 'journey', 'joy', 'judg', 'jump', 'keep', 'kelli', 'kept', 'key', 'kick', 'kid', 'kill', 'killer', 'kind', 'king', 'knew', 'know', 'known', 'la', 'lack', 'ladi', 'lame', 'land', 'languag', 'larg', 'last', 'late', 'later', 'latter', 'laugh', 'laughabl', 'law', 'lead', 'leader', 'learn', 'least', 'leav', 'led', 'lee', 'left', 'legend', 'length', 'lesbian', 'less', 'lesson', 'let', 'level', 'lie', 'life', 'light', 'likabl', 'like', 'limit', 'line', 'list', 'listen', 'liter', 'littl', 'live', 'load', 'local', 'locat', 'london', 'lone', 'long', 'longer', 'look', 'loos', 'lord', 'lose', 'lost', 'lot', 'loud', 'love', 'lover', 'low', 'machin', 'mad', 'made', 'magic', 'magnific', 'main', 'mainli', 'major', 'make', 'maker', 'male', 'man', 'manag', 'mani', 'manner', 'mari', 'mark', 'market', 'marri', 'marriag', 'martin', 'mask', 'master', 'masterpiec', 'match', 'materi', 'matter', 'may', 'mayb', 'mean', 'meant', 'mediocr', 'meet', 'member', 'memor', 'memori', 'men', 'mental', 'mention', 'mere', 'mess', 'messag', 'met', 'michael', 'middl', 'might', 'mile', 'militari', 'million', 'mind', 'minor', 'minut', 'miss', 'mistak', 'mix', 'model', 'modern', 'mom', 'moment', 'money', 'monster', 'month', 'mood', 'moor', 'moral', 'mostli', 'mother', 'motion', 'motiv', 'mountain', 'mouth', 'move', 'movi', 'mr', 'ms', 'much', 'murder', 'music', 'must', 'mysteri', 'nake', 'name', 'narr', 'narrat', 'nasti', 'nation', 'natur', 'near', 'nearli', 'need', 'neg', 'neither', 'never', 'new', 'next', 'nice', 'night', 'nightmar', 'nobodi', 'noir', 'nomin', 'non', 'none', 'nonsens', 'normal', 'notabl', 'note', 'noth', 'notic', 'novel', 'nowher', 'nuditi', 'number', 'numer', 'object', 'obsess', 'obviou', 'obvious', 'occasion', 'occur', 'odd', 'offer', 'offic', 'often', 'oh', 'ok', 'okay', 'old', 'older', 'one', 'onto', 'open', 'opera', 'opinion', 'opportun', 'opposit', 'order', 'origin', 'oscar', 'other', 'otherwis', 'outsid', 'outstand', 'overal', 'pace', 'pack', 'paid', 'pain', 'paint', 'pair', 'parent', 'pari', 'park', 'parodi', 'part', 'parti', 'particular', 'particularli', 'pass', 'passion', 'past', 'pathet', 'patient', 'paul', 'pay', 'peopl', 'perfect', 'perfectli', 'perform', 'perhap', 'period', 'person', 'peter', 'phone', 'photograph', 'photographi', 'physic', 'pick', 'pictur', 'piec', 'pilot', 'piti', 'place', 'plain', 'plan', 'plane', 'planet', 'play', 'player', 'pleas', 'pleasur', 'plenti', 'plot', 'plu', 'point', 'pointless', 'polic', 'polit', 'poor', 'poorli', 'pop', 'popular', 'porn', 'portray', 'posit', 'possess', 'possibl', 'post', 'potenti', 'power', 'practic', 'predict', 'prefer', 'premis', 'prepar', 'presenc', 'present', 'pretti', 'previou', 'price', 'prison', 'probabl', 'problem', 'process', 'produc', 'product', 'profession', 'program', 'progress', 'project', 'promis', 'protagonist', 'protect', 'prove', 'provid', 'public', 'pull', 'pure', 'purpos', 'push', 'put', 'qualiti', 'queen', 'question', 'quick', 'quickli', 'quit', 'race', 'radio', 'rais', 'random', 'rang', 'rape', 'rare', 'rate', 'rather', 'ray', 'reach', 'reaction', 'read', 'readi', 'real', 'realist', 'realiti', 'realiz', 'realli', 'reason', 'receiv', 'recent', 'recogn', 'recommend', 'record', 'red', 'redeem', 'refer', 'reflect', 'regard', 'rel', 'relat', 'relationship', 'releas', 'religi', 'remain', 'remak', 'remark', 'rememb', 'remind', 'remot', 'rent', 'repeat', 'replac', 'report', 'repres', 'requir', 'resembl', 'respect', 'respons', 'rest', 'result', 'return', 'reveal', 'reveng', 'review', 'rich', 'richard', 'ride', 'ridicul', 'right', 'ring', 'rip', 'rise', 'river', 'road', 'rob', 'robert', 'rock', 'role', 'roll', 'romanc', 'romant', 'room', 'round', 'ruin', 'rule', 'run', 'russian', 'ryan', 'sad', 'sadli', 'said', 'sam', 'satisfi', 'save', 'saw', 'say', 'scare', 'scari', 'scene', 'sceneri', 'school', 'sci', 'scienc', 'scientist', 'score', 'scott', 'scream', 'screen', 'screenplay', 'script', 'sea', 'seagal', 'sean', 'search', 'season', 'seat', 'second', 'secret', 'see', 'seek', 'seem', 'seemingli', 'seen', 'self', 'sell', 'send', 'sens', 'sentiment', 'sequel', 'sequenc', 'seri', 'serial', 'seriou', 'serious', 'serv', 'set', 'seven', 'sever', 'sex', 'sexi', 'sexual', 'shadow', 'shame', 'share', 'shine', 'ship', 'shock', 'shoot', 'shop', 'short', 'shot', 'show', 'shown', 'sick', 'side', 'sight', 'sign', 'signific', 'silent', 'silli', 'similar', 'simpl', 'simpli', 'sinc', 'sing', 'singer', 'singl', 'sister', 'sit', 'situat', 'six', 'skill', 'skip', 'slasher', 'sleep', 'slightli', 'slow', 'slowli', 'small', 'smart', 'smile', 'smith', 'social', 'societi', 'soldier', 'solid', 'somebodi', 'somehow', 'someon', 'someth', 'sometim', 'somewhat', 'somewher', 'son', 'song', 'soon', 'sorri', 'sort', 'soul', 'sound', 'soundtrack', 'south', 'space', 'speak', 'special', 'spend', 'spent', 'spirit', 'spoil', 'spoiler', 'sport', 'spot', 'st', 'stage', 'stand', 'standard', 'star', 'start', 'state', 'station', 'stay', 'steal', 'step', 'stephen', 'stereotyp', 'steve', 'steven', 'stewart', 'stick', 'still', 'stone', 'stop', 'store', 'stori', 'storylin', 'straight', 'strang', 'street', 'strike', 'strong', 'struggl', 'stuck', 'student', 'studi', 'studio', 'stuff', 'stun', 'stunt', 'stupid', 'style', 'sub', 'subject', 'subtl', 'success', 'suck', 'suddenli', 'suffer', 'suggest', 'suicid', 'suit', 'summer', 'super', 'superb', 'support', 'suppos', 'sure', 'surpris', 'surprisingli', 'surround', 'surviv', 'suspect', 'suspens', 'sweet', 'symbol', 'system', 'take', 'taken', 'tale', 'talent', 'talk', 'tape', 'tast', 'teach', 'teacher', 'team', 'tear', 'technic', 'teen', 'teenag', 'televis', 'tell', 'ten', 'tend', 'tension', 'term', 'terribl', 'terrif', 'test', 'th', 'thank', 'theater', 'theme', 'thin', 'thing', 'think', 'third', 'thoroughli', 'though', 'thought', 'three', 'thrill', 'thriller', 'throughout', 'throw', 'thrown', 'thu', 'tie', 'tim', 'time', 'tire', 'titl', 'today', 'togeth', 'told', 'tom', 'tone', 'toni', 'took', 'top', 'tortur', 'total', 'touch', 'tough', 'toward', 'town', 'track', 'tradit', 'tragedi', 'tragic', 'trailer', 'train', 'transform', 'trap', 'trash', 'travel', 'treat', 'tri', 'trick', 'trip', 'troubl', 'true', 'truli', 'trust', 'truth', 'tune', 'turn', 'tv', 'twice', 'twist', 'two', 'type', 'typic', 'ugli', 'ultim', 'unbeliev', 'understand', 'unfortun', 'uniqu', 'univers', 'unknown', 'unless', 'unlik', 'unusu', 'upon', 'us', 'use', 'usual', 'utterli', 'valu', 'vampir', 'van', 'variou', 'version', 'victim', 'video', 'view', 'viewer', 'villag', 'villain', 'violenc', 'violent', 'vision', 'visit', 'visual', 'voic', 'vote', 'wait', 'walk', 'wall', 'want', 'war', 'warm', 'warn', 'wast', 'watch', 'water', 'way', 'weak', 'weapon', 'wear', 'week', 'weird', 'well', 'went', 'west', 'western', 'whatev', 'whatsoev', 'whether', 'white', 'whole', 'whose', 'wide', 'wife', 'wild', 'will', 'william', 'win', 'wind', 'window', 'wise', 'wish', 'wit', 'witch', 'within', 'without', 'woman', 'women', 'wonder', 'wood', 'word', 'work', 'world', 'worri', 'wors', 'worst', 'worth', 'worthi', 'would', 'wow', 'write', 'writer', 'written', 'wrong', 'wrote', 'ye', 'yeah', 'year', 'yet', 'york', 'young', 'younger', 'youth', 'zero', 'zombi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e98ed1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 1500)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea987977",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data2.iloc[:, 1].values # dependent variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc8e777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596cfac",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d7e96e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7764285714285715"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) \n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the results\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978912b9",
   "metadata": {},
   "source": [
    "## KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a41fc4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7171428571428572"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad274ca8",
   "metadata": {},
   "source": [
    "## Support Vector Machine SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df549f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464285714285714"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the results\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc7735",
   "metadata": {},
   "source": [
    "## Decision Tree Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2364f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7171428571428572"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6f3f1",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bd84cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7742857142857142"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Splitting the dataset into the Training set and Test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10, criterion = 'entropy')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# Evaluate the results\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d333ea",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87f6f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "86c15a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "faf16554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c28100f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 2500)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0d9b2",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d646dba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7721428571428571"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the dataset into the Training set and Test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10, criterion = 'entropy')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# Evaluate the results\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5d7ea",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe151315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017142857142857"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7c50a",
   "metadata": {},
   "source": [
    "## Support Vector Machine SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a0cb3d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8778571428571429"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predicting the Test set results \n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the results\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000c3a4",
   "metadata": {},
   "source": [
    "## KNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "558865fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7028571428571428"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5, metric = 'minkowski', p =2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results \n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# Evaluate the results\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6633cc",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "842f0733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7957142857142857"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) \n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the results\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f74a2c",
   "metadata": {},
   "source": [
    "### Performance of all implemented model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0de07b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------+------------+\n",
      "|    | Model_Name                       |   Accuracy |\n",
      "+====+==================================+============+\n",
      "|  0 | Bag_of_words_Random Forest Model |   0.774286 |\n",
      "+----+----------------------------------+------------+\n",
      "|  1 | Bag_of_words_Decision Tree       |   0.717143 |\n",
      "+----+----------------------------------+------------+\n",
      "|  2 | Bag_of_words_SVM                 |   0.846429 |\n",
      "+----+----------------------------------+------------+\n",
      "|  3 | Bag_of_words_KNN                 |   0.717143 |\n",
      "+----+----------------------------------+------------+\n",
      "|  4 | Bag_of_words_Naive Bayes         |   0.776429 |\n",
      "+----+----------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data_bag_of_words = [[\"Bag_of_words_Random Forest Model\", 0.7742857142857142], \n",
    "        [\"Bag_of_words_Decision Tree\", 0.7171428571428572], \n",
    "        [\"Bag_of_words_SVM\", 0.8464285714285714], \n",
    "        [\"Bag_of_words_KNN\", 0.7171428571428572],\n",
    "        [\"Bag_of_words_Naive Bayes\", 0.7764285714285715]]\n",
    "\n",
    "col_names = [\"Model_Name\", \"Accuracy\"]\n",
    "\n",
    "print(tabulate(data_bag_of_words, headers=col_names, tablefmt=\"grid\", showindex=\"always\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f888bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------------+------------+\n",
      "|    | Model_Name                        |   Accuracy |\n",
      "+====+===================================+============+\n",
      "|  0 | TF_IDF_Random Forest Model        |   0.772143 |\n",
      "+----+-----------------------------------+------------+\n",
      "|  1 | TF_IDF_Bag_of_words_Decision Tree |   0.701714 |\n",
      "+----+-----------------------------------+------------+\n",
      "|  2 | TF_IDF_Bag_of_words_SVM           |   0.877857 |\n",
      "+----+-----------------------------------+------------+\n",
      "|  3 | TF_IDF_Bag_of_words_KNN           |   0.702857 |\n",
      "+----+-----------------------------------+------------+\n",
      "|  4 | TF_IDF_Bag_of_words_Naive Bayes   |   0.795714 |\n",
      "+----+-----------------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "data_TF_IDF = [[\"TF_IDF_Random Forest Model\", 0.7721428571428571], \n",
    "        [\"TF_IDF_Bag_of_words_Decision Tree\", 0.7017142857142857], \n",
    "        [\"TF_IDF_Bag_of_words_SVM\", 0.8778571428571429], \n",
    "        [\"TF_IDF_Bag_of_words_KNN\", 0.7028571428571428],\n",
    "        [\"TF_IDF_Bag_of_words_Naive Bayes\", 0.7957142857142857]]\n",
    "\n",
    "col_names = [\"Model_Name\", \"Accuracy\"]\n",
    "\n",
    "print(tabulate(data_TF_IDF, headers=col_names, tablefmt=\"grid\", showindex=\"always\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
